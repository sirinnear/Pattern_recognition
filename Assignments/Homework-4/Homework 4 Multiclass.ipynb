{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 Softmax Function\n",
    "\n",
    "We learned a ECOC in class which is a general way of turning any binary classifier into multiclass classifier. In this problem we will learn how to naturally extends logistics regression into multiclass classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1) Softmax Function.\n",
    "\n",
    "In logistic regression we model the probability for data $\\vec{x}$ to be of class $y$ as\n",
    "\n",
    "$$\n",
    "    P(y | \\vec{x};\\vec{w}) = \\begin{cases}\n",
    "    \\theta(\\vec{w} \\cdot \\vec{x}) & \\text{if } y = +1\\\\\n",
    "    1 - \\theta(\\vec{w} \\cdot \\vec{x}) & \\text{if } y = -1\n",
    "    \\end{cases}\n",
    "$$\n",
    "where $\\theta(s) = \\frac{1}{1+\\exp(-s)} $\n",
    "\n",
    "Now we want to extends it to multiclass problem. This means that $y$ can attain more than two values eg: ($1,2,3,\\ldots,k$). Consider the softmax function\n",
    "$$\n",
    "P(y | \\vec{x}; \\vec{w_1}, \\vec{w_2}, \\ldots, \\vec{w_k}) = \\frac{\\exp(\\vec{w_y}\\cdot\\vec{x})}{\\sum_{i=1\\ldots k}{\\exp(\\vec{w_i}\\cdot\\vec{x})}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 Knowing the Symbols\n",
    "\n",
    "Given \n",
    "- $\\vec{w_1} = [1,2,3]$\n",
    "- $\\vec{w_2} = [4,5,6]$\n",
    "- $\\vec{w_3} = [7,8,9]$\n",
    "- $\\vec{x} = [1,-2,-3]$\n",
    "\n",
    "Find $P(1|\\vec{x}; W)$, $P(2|\\vec{x}; W)$, $P(3|\\vec{x}; W)$.\n",
    "\n",
    "Where $W = \\{ \\vec{w_1}, \\vec{w_2}, \\vec{w_3} \\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [1., 2., 3.]\n",
    "w2 = [4., 5., 6.]\n",
    "w3 = [7., 8., 9.]\n",
    "w = np.vstack((w1, w2, w3))\n",
    "\n",
    "x = np.array([1., -2., -3.])\n",
    "\n",
    "def calculate_prob(w, x, y):\n",
    "    denom = [np.exp(np.dot(x, w[i])) for i in range(len(w))]\n",
    "    return np.exp(np.dot(x, w[y-1])) / np.sum(denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.14421235332821e-06\n",
      "3.775134544279098e-11\n",
      "2.3195228302435696e-16\n",
      "6.144250104905605e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.],\n",
       "       [7., 8., 9.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(len(w)):\n",
    "    sum += np.exp(np.dot(x, w[i]))\n",
    "    print(np.exp(np.dot(x, w[i])))\n",
    "print(sum)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1: 0.9999938557876469\n",
      "P2: 6.1441746019827675e-06\n",
      "P3: 3.7751113490507965e-11\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    prob = calculate_prob(w, x, i) \n",
    "    print(\"P{}: {}\".format(i, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "Explain/proof why the range of \n",
    "$$\n",
    "P(y | \\vec{x}; \\vec{w_1}, \\vec{w_2}, \\ldots, \\vec{w_k}) = \\frac{\\exp(\\vec{w_y}\\cdot\\vec{x})}{\\sum_{i=1\\ldots k}{\\exp(\\vec{w_i}\\cdot\\vec{x})}}\n",
    "$$\n",
    "is between $(0,1)$. Short explanation is suffice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The denominator is always >= 1\n",
    "# If only 1 w vector exists, P = 1\n",
    "# If > 1 w vector exists, P < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data $y^{(i)}$ and $\\vec{x}^{(i)}$ for $i=1\\ldots n$.\n",
    "\n",
    "Write the likelihood of obtaining this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lh = P(y1|x1;w) * P(y2|x2;w) * ... * P(yn|xn;w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(w, x, y, n):\n",
    "    prob = [calculate_prob(x, x[i], y[i]) for i in range(n)]\n",
    "    return np.product(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4\n",
    "\n",
    "Suppose we want to use log-likelihood as our cost function(for minimize). Write down the cost function for obtaining the best $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w):\n",
    "    prob = [calculate_prob(x, x[i], y[i]) for i in range(n)]\n",
    "    neg_log_lh = -np.sum(np.log(np.array(prob)))\n",
    "    return neg_log_lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given what you found in 4. Consider the [Wine Dataset](http://archive.ics.uci.edu/ml/datasets/Wine). We want to build a system to predict the wine origin (1,2,3) which is at the first column for the datafile. The data is in **data/wine.csv**.\n",
    "\n",
    "**Your Task**\n",
    "\n",
    "- Build a multiclass classifier  using softmax function.\n",
    "\n",
    "- Build show the [confusion matrix](http://www2.cs.uregina.ca/~dbd/cs831/notes/confusion_matrix/confusion_matrix.html) based on the train data. (This should be a 3x3 matrix)\n",
    "\n",
    "- Feature such as hue will need a non-linear transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1     2     3     4    5     6     7     8     9      10    11  \\\n",
       "0     1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29   5.64  1.04   \n",
       "1     1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28   4.38  1.05   \n",
       "2     1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81   5.68  1.03   \n",
       "3     1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18   7.80  0.86   \n",
       "4     1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82   4.32  1.04   \n",
       "..   ..    ...   ...   ...   ...  ...   ...   ...   ...   ...    ...   ...   \n",
       "173   3  13.71  5.65  2.45  20.5   95  1.68  0.61  0.52  1.06   7.70  0.64   \n",
       "174   3  13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41   7.30  0.70   \n",
       "175   3  13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35  10.20  0.59   \n",
       "176   3  13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46   9.30  0.60   \n",
       "177   3  14.13  4.10  2.74  24.5   96  2.05  0.76  0.56  1.35   9.20  0.61   \n",
       "\n",
       "       12    13  \n",
       "0    3.92  1065  \n",
       "1    3.40  1050  \n",
       "2    3.17  1185  \n",
       "3    3.45  1480  \n",
       "4    2.93   735  \n",
       "..    ...   ...  \n",
       "173  1.74   740  \n",
       "174  1.56   750  \n",
       "175  1.56   835  \n",
       "176  1.62   840  \n",
       "177  1.60   560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/wine.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 [Usenet News](https://en.wikipedia.org/wiki/Usenet_newsgroup) Classifications\n",
    "\n",
    "At the dawn of internet. There exists a thing called usenet where people discuss various thing. Think about it as ancient reddit.\n",
    "\n",
    "In this problem we will be learning about how to deal with large($n=20$) number of classes with ECOC.\n",
    "\n",
    "The data is at **data/news_topic_train.csv.gz** and **data/news_topic_train.csv.gz**. You can just have panda read gzip file. No need to unzip it. The data is vectorized and cleaned data based on [20Newsgroups Dataset](http://qwone.com/~jason/20Newsgroups/). Read prepare.ipynb if you want to see what exactly I did in cleaning the data.\n",
    "\n",
    "Our goal for this problem is to build a classifier which can read the discussion and figure out which topic the discussion belongs to.\n",
    "\n",
    "**Don't forget to pad the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Why don't we use exhaustive matrix for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Supposed that we want to use 200 classifiers to do this. We could generate the code matrix randomly many many times.\n",
    "\n",
    "Which criteria should we use in selecting the best code matrix?\n",
    "\n",
    "Optional: you can get a better code matrix by doing hill climbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Build and ECOC classifier with 200 classifiers using **data/news_topic_test.csv.gz** and test the performance on **data/news_topic_train.csv.gz**(get the accuracy). Make sure you use a decent code table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
